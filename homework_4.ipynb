{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fad453",
   "metadata": {},
   "source": [
    "# Домашнее задание № 4. Языковые модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d056af4",
   "metadata": {},
   "source": [
    "## Задание 1 (8 баллов)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f532a8",
   "metadata": {},
   "source": [
    "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de743d1d",
   "metadata": {},
   "source": [
    "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
    "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
    "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
    "\n",
    "\n",
    "Подсказки:  \n",
    "    - нужно будет добавить еще один тэг <start>  \n",
    "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
    "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d078056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize as razdel_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6afcef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = open(r\"C:\\Users\\marus\\Documents\\Вышка\\compling_nlp_hse_course\\notebooks\\lm_intro\\lenta.txt\", encoding = 'utf-8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f699df4f",
   "metadata": {},
   "source": [
    "### Генератор \n",
    "NOTE: По какой-то причине в гитхабе не отображаются теги start и end - ниже везде, где кавычки с пустотой, стоит один из этих тегов" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afe70f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    normalized_text = [word.text.strip(punctuation) for word \\\n",
    "                                                            in razdel_tokenize(text)]\n",
    "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20]\n",
    "    \n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "572c9b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] + ['<end>'] for text in sent_tokenize(news[:5000000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b9191530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для подсчета n-грамм\n",
    "\n",
    "def ngrammer(tokens, n):\n",
    "    ngrams = []\n",
    "    for i in range(0,len(tokens)-n+1):\n",
    "        ngrams.append(' '.join(tokens[i:i+n]))\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7d14394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Униграммы, биграммы и триграммы для новостей\n",
    "\n",
    "unigrams_news = Counter()\n",
    "bigrams_news = Counter()\n",
    "trigrams_news = Counter()\n",
    "\n",
    "for sentence in sentences_news:\n",
    "    unigrams_news.update(sentence)\n",
    "    bigrams_news.update(ngrammer(sentence, 2))\n",
    "    trigrams_news.update(ngrammer(sentence, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa56d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заполняем матрицу\n",
    "\n",
    "matrix_news = lil_matrix((len(bigrams_news), \n",
    "                         len(unigrams_news)))\n",
    "\n",
    "id2word_news_1 = list(unigrams_news)\n",
    "word2id_news_1 = {word:i for i, word in enumerate(id2word_news_1)}\n",
    "id2word_news_2 = list(bigrams_news)\n",
    "word2id_news_2 = {word:i for i, word in enumerate(id2word_news_2)}\n",
    "\n",
    "for ngram in trigrams_news:\n",
    "    word1, word2, word3 = ngram.split()\n",
    "    bigram = ' '.join([word1, word2])\n",
    "    unigram = word3\n",
    "    matrix_news[word2id_news_2[bigram], word2id_news_1[unigram]] =  (trigrams_news[ngram]/bigrams_news[bigram])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7c1f7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для генерации текстов\n",
    "\n",
    "def generate(matrix, id2word_1, word2id_2, n=100, start='<start> <start>'):\n",
    "    text = []\n",
    "    current_idx = word2id_2[start]\n",
    "    current_bigram = start\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx].toarray()[0])\n",
    "        text.append(id2word_1[chosen])\n",
    "        current_bigram = f'{current_bigram.split()[1]} {id2word_1[chosen]}'\n",
    "        \n",
    "        if id2word_1[chosen] == '<end>':\n",
    "            chosen = word2id_2[start]\n",
    "            current_bigram = start\n",
    "            \n",
    "        current_idx = word2id_2[current_bigram]\n",
    "\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "515c7129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "в то же время такие темпы роста населения планеты продолжают увеличиваться \n",
      " мы не будем немедленно делать вывод о целесообразности размещения подобных табло на других вокзалах ирландии глава всероссийского центра изучения общественного мнения на различные актуальные темы \n",
      " он признал что его фирма mabetex действительно помогала ельцину и его структурные службы увд центрального северо-восточного южного административных округов москвы города зеленограда а также начало конца старых масс-медиа а также населенных пунктов либо разрушены либо отрезаны стихией от внешнего мира \n",
      " в этой связи нил аберкромби отметил построение в россии связана с тем как сообщает риа новости \n",
      " в то время как\n"
     ]
    }
   ],
   "source": [
    "print(generate(matrix_news, id2word_news_1, word2id_news_2).replace('<end>', '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586ab80e",
   "metadata": {},
   "source": [
    "### Перплексия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "75789fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отложенные предложения из корпуса\n",
    "\n",
    "probas_news = [normalize(text) for text in sent_tokenize(news[-3000:])][0:39]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c69b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сгенерированные тексты\n",
    "text_generated = generate(matrix_news, id2word_news_1, word2id_news_2).replace('<end>', '\\n').split('\\n')[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7787f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(logp, N):\n",
    "    return np.exp((-1/N) * logp)\n",
    "\n",
    "# Перплексия на корпусе\n",
    "def compute_joint_proba(text, word_probas):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for word in tokens:\n",
    "        if word in word_probas:\n",
    "            prob += (np.log(word_probas[word]))\n",
    "        else:\n",
    "            prob += np.log(2e-4)\n",
    "    \n",
    "    return prob, len(tokens)\n",
    "\n",
    "# перплексия на биграмной модели\n",
    "\n",
    "def compute_join_proba_markov_assumption_bigrams(text, word_counts, bigram_counts):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for ngram in ngrammer(['<start>'] + tokens + ['<end>'], 2):\n",
    "        word1, word2 = ngram.split()\n",
    "        if word1 in word_counts and ngram in bigram_counts:\n",
    "            prob += np.log(bigram_counts[ngram]/word_counts[word1])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return prob, len(tokens)\n",
    "\n",
    "# перплексия на триграмной модели\n",
    "\n",
    "def compute_join_proba_markov_assumption_trigrams(text, bigram_counts, trigram_counts):\n",
    "    prob = 0\n",
    "    tokens = normalize(text)\n",
    "    for ngram in ngrammer(['<start>'] + tokens + ['<end>'], 3):\n",
    "        word1, word2, word3 = ngram.split()\n",
    "        bigram = ' '.join([word1, word2])\n",
    "        unigram = word3\n",
    "        if bigram in bigram_counts and ngram in trigram_counts:\n",
    "            prob += np.log(trigram_counts[ngram]/bigram_counts[bigram])\n",
    "        else:\n",
    "            prob += np.log(2e-5)\n",
    "    \n",
    "    return prob, len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "86f69fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplx1 = [perplexity(*compute_joint_proba(i, probas_news)) for i in text_generated]\n",
    "perplx2 = [perplexity(*compute_join_proba_markov_assumption_bigrams(i, unigrams_news, bigrams_news)) for i in text_generated]\n",
    "perplx3 = [perplexity(*compute_join_proba_markov_assumption_trigrams(i, bigrams_news, trigrams_news)) for i in text_generated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d78265d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя перплексия униграмной модели - 5000.000000000006\n",
      "Средняя перплексия биграмной модели - 51.27200325900432\n",
      "Средняя перплексия триграмной модели - 2.9045891985380035\n"
     ]
    }
   ],
   "source": [
    "print(f'Средняя перплексия униграмной модели - {np.mean(perplx1)}')\n",
    "print(f'Средняя перплексия биграмной модели - {np.mean(perplx2)}')\n",
    "print(f'Средняя перплексия триграмной модели - {np.mean(perplx3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257613e",
   "metadata": {},
   "source": [
    "Ожидаемо, модель на биграммах более точна, чем модель на униграммах, а модель на триграммах - точнее и униграмной, и биграмной."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a8dd5",
   "metadata": {},
   "source": [
    "## Задание № 2* (2 балла). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36c44b",
   "metadata": {},
   "source": [
    "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b1bd8",
   "metadata": {},
   "source": [
    "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2cf844",
   "metadata": {},
   "source": [
    "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b10ff4d",
   "metadata": {},
   "source": [
    "Незнакомые слова могут встретиться в открытой лексической системе. Чтобы модель работала с потенциально незнакомыми для себя словами, эти слова моделируются с помощью псевдослова <UNK>, вероятность которого можно посчитать двумя способами:\n",
    "1. Выбрать заранее фиксированный список слов. Все слова, которые в него не входят, заменить на <UNK>. Считать <UNK> просто одним из слов и, соответственно, рассчитывать ему вероятность так же, как и другим обычным словам.\n",
    "    \n",
    "2. Если нет возможности составить фиксированный список слов, можно заменить на <UNK> слова, которые встречаются реже, чем n раз (n – некое маленькое число, зависит от общего размера словаря) - <UNK> опять же будет рассматриваться как обычное слово."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d1c152",
   "metadata": {},
   "source": [
    "2. Что такое сглаживание (smoothing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a6a272",
   "metadata": {},
   "source": [
    "Сглаживание - это метод, который используется для того, чтобы избежать нулевой вероятности для тех случаев, когда слово встречается в незнакомом для модели контексте. Для этого часть вероятностной массы забирается от самых частотных событий и резервируется для событий, которые еще не встречались. Благодаря этому такие события получают не нулевую вероятность, а часть от \"зарезервированной\" вероятности.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
