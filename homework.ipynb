{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20f786e",
   "metadata": {},
   "source": [
    "# Домашнее задание № 2. Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf72d19",
   "metadata": {},
   "source": [
    "## Задание 1 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a045e99",
   "metadata": {},
   "source": [
    "У векторайзеров в sklearn есть встроенная токенизация на регулярных выражениях. Найдите способо заменить её на кастомную токенизацию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4d453",
   "metadata": {},
   "source": [
    "Обучите векторайзер с дефолтной токенизацией и с токенизацией razdel.tokenize. Обучите классификатор (любой) с каждым из векторизаторов. Сравните метрики и выберете победителя. \n",
    "\n",
    "(в вашей тетрадке должен быть код обучения и все метрики; если вы сдаете в .py файлах то сохраните полученные метрики в отдельном файле или в комментариях)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d2477b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dcdb7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка с импортами - общая для всех заданий\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from razdel import tokenize\n",
    "\n",
    "import string\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4014f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\marus\\Documents\\Вышка\\compling_nlp_hse_course\\notebooks\\bow\\labeled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fdf6249",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.1, shuffle=True)\n",
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)\n",
    "\n",
    "y_train = train.toxic.values\n",
    "y_test = test.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da3a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция с токенизацией из razdel.tokenize\n",
    "\n",
    "def tok(text):\n",
    "    new_sent = [j.text for j in list(tokenize(text))]\n",
    "    return new_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a07af34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.84      0.81       943\n",
      "         1.0       0.65      0.57      0.60       499\n",
      "\n",
      "    accuracy                           0.74      1442\n",
      "   macro avg       0.72      0.70      0.71      1442\n",
      "weighted avg       0.74      0.74      0.74      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.1 - Count Vectorizer с собственной токенизацией\n",
    "\n",
    "vec1_1 = CountVectorizer()\n",
    "X1_1_train = vec1_1.fit_transform(train.comment)\n",
    "X1_1_test = vec1_1.transform(test.comment) \n",
    "\n",
    "# Везде используется классификатор Decision Tree Classifier\n",
    "\n",
    "clf1_1 = DecisionTreeClassifier()\n",
    "clf1_1.fit(X1_1_train, y_train)\n",
    "\n",
    "y_preds1_1 = clf1_1.predict(X1_1_test)\n",
    "\n",
    "print(classification_report(y_test, y_preds1_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2120fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.85      0.82       943\n",
      "         1.0       0.67      0.59      0.63       499\n",
      "\n",
      "    accuracy                           0.76      1442\n",
      "   macro avg       0.73      0.72      0.72      1442\n",
      "weighted avg       0.75      0.76      0.75      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1.2 Count Vectorizer с токенизацией razdel.tokenize\n",
    "\n",
    "vec1_2 = CountVectorizer(tokenizer = tok)\n",
    "X1_2_train = vec1_2.fit_transform(train.comment)\n",
    "X1_2_test = vec1_2.transform(test.comment) \n",
    "\n",
    "clf1_2 = DecisionTreeClassifier()\n",
    "clf1_2.fit(X1_2_train, y_train)\n",
    "\n",
    "y_preds1_2 = clf1_2.predict(X1_2_test)\n",
    "\n",
    "print(classification_report(y_test, y_preds1_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdd1e0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.82      0.80       943\n",
      "         1.0       0.63      0.57      0.60       499\n",
      "\n",
      "    accuracy                           0.73      1442\n",
      "   macro avg       0.71      0.70      0.70      1442\n",
      "weighted avg       0.73      0.73      0.73      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.1 - TfIdf с собственной токенизацией\n",
    "\n",
    "vec2_1 = TfidfVectorizer()\n",
    "X2_1_train = vec2_1.fit_transform(train.comment)\n",
    "X2_1_test = vec2_1.transform(test.comment) \n",
    "\n",
    "clf2_1 = DecisionTreeClassifier()\n",
    "clf2_1.fit(X2_1_train, y_train)\n",
    "\n",
    "y_preds2_1 = clf2_1.predict(X2_1_test)\n",
    "\n",
    "print(classification_report(y_test, y_preds2_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2012eea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.83      0.80       943\n",
      "         1.0       0.63      0.56      0.60       499\n",
      "\n",
      "    accuracy                           0.74      1442\n",
      "   macro avg       0.71      0.69      0.70      1442\n",
      "weighted avg       0.73      0.74      0.73      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.2 - TfIdf с токенизацией razdel.tokenize\n",
    "\n",
    "vec2_2 = TfidfVectorizer(tokenizer = tok)\n",
    "\n",
    "X2_2_train = vec2_2.fit_transform(train.comment)\n",
    "X2_2_test = vec2_2.transform(test.comment) \n",
    "\n",
    "clf2_2 = DecisionTreeClassifier()\n",
    "clf2_2.fit(X2_2_train, y_train)\n",
    "\n",
    "y_preds2_2 = clf2_2.predict(X2_2_test)\n",
    "\n",
    "print(classification_report(y_test, y_preds2_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e443c16c",
   "metadata": {},
   "source": [
    "**Выводы**\n",
    "\n",
    "Модель с использованием Count Vectorizer с токенизацией razdel tokenize показала наивысший результат: f1 score 0.82 для нетоксичных и 0.63 для токсичных комментариев - результаты выше, чем у всех остальных моделей.\n",
    "\n",
    "Здесь видно, что модели с векторизаторами, использующими кастомный токенизатор, показали более высокий результат, чем с векторизаторами с собственным токенизатором, но в целом результаты сопоставимы, и самый большой отрыв - 0.03 (f1 score для токсичных комментариев, в пользу кастомного токенизатора).\n",
    "\n",
    "Как я заметила, razdel.tokenize, в отличие от встроенного токенизатора, выделяет знаки пунктуации как токены, что может иметь значение при анализе тональности (напр., использование большого числа восклицательных знаков). Видимо, именно это позволило моделям с кастомным токенизатором показать более высокий f1 score и accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c896c",
   "metadata": {},
   "source": [
    "## Задание 2 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd27a3",
   "metadata": {},
   "source": [
    "Преобразуйте таблицу с абсолютными частотностями в семинарской тетрадке в таблицу с tfidf значениями. (Таблица - https://i.ibb.co/r5Nc2HC/abs-bow.jpg) Формула tfidf есть в семинаре на картнике с пояснениями на английском. \n",
    "Считать нужно в питоне. Формат итоговой таблицы может быть любым, главное, чтобы был код и можно было воспроизвести вычисления. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e05806a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['я и ты', 'ты и я', 'я, я и только я', 'только не я', 'он']\n",
    "tokenized_sentences = []\n",
    "\n",
    "for sntnc in sentences:\n",
    "    words = []\n",
    "    a = list(tokenize(sntnc))\n",
    "    for j in a:\n",
    "        token = j.text\n",
    "        if token not in string.punctuation:\n",
    "            words.append(token)\n",
    "    tokenized_sentences.append(words)\n",
    "\n",
    "\n",
    "# здесь считаются tf и употребление слов в документах, нужное для idf\n",
    "\n",
    "each_sentence = []\n",
    "idf = {}\n",
    "for sentence in tokenized_sentences:\n",
    "    tf = {}\n",
    "    for token in sentence:\n",
    "        if token in tf.keys():\n",
    "            tf[token] +=1\n",
    "        else:\n",
    "            tf[token] = 1\n",
    "            if token in idf.keys():\n",
    "                idf[token] +=1\n",
    "            else:\n",
    "                idf[token] = 1\n",
    "                \n",
    "    for item in tf.keys():\n",
    "        tf[item] = tf[item]/len(sentence)\n",
    "\n",
    "    each_sentence.append(tf)\n",
    "\n",
    "# здесь считается финальная формула idf\n",
    "for i in idf.keys():\n",
    "    idf[i] = (math.log(len(sentences)/idf[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07ed4e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'я': 0.3333333333333333, 'и': 0.3333333333333333, 'ты': 0.3333333333333333},\n",
       " {'ты': 0.3333333333333333, 'и': 0.3333333333333333, 'я': 0.3333333333333333},\n",
       " {'я': 0.6, 'и': 0.2, 'только': 0.2},\n",
       " {'только': 0.3333333333333333,\n",
       "  'не': 0.3333333333333333,\n",
       "  'я': 0.3333333333333333},\n",
       " {'он': 1.0}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посчитанные tf\n",
    "\n",
    "each_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c11a145a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'я': 0.22314355131420976,\n",
       " 'и': 0.5108256237659907,\n",
       " 'ты': 0.9162907318741551,\n",
       " 'только': 0.9162907318741551,\n",
       " 'не': 1.6094379124341003,\n",
       " 'он': 1.6094379124341003}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посчитанные idf\n",
    "\n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e8206faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataf = pd.DataFrame([idf])\n",
    "tfidf = pd.DataFrame(0.0, index = sentences, columns = idf.keys())\n",
    "tfidf = tfidf.reindex(columns = ['я', 'ты', 'и', 'только', 'не', 'он'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "54e0ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "row = 0\n",
    "for sentence in each_sentence:\n",
    "    for word in sentence.keys():\n",
    "        for k in idf.keys():\n",
    "            if word == k:\n",
    "                tf = sentence[word]\n",
    "                tfidf[word][row] = tf * idf[word]\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7c41f604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>я</th>\n",
       "      <th>ты</th>\n",
       "      <th>и</th>\n",
       "      <th>только</th>\n",
       "      <th>не</th>\n",
       "      <th>он</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>я и ты</th>\n",
       "      <td>0.074381</td>\n",
       "      <td>0.30543</td>\n",
       "      <td>0.170275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ты и я</th>\n",
       "      <td>0.074381</td>\n",
       "      <td>0.30543</td>\n",
       "      <td>0.170275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>я, я и только я</th>\n",
       "      <td>0.133886</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.102165</td>\n",
       "      <td>0.183258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>только не я</th>\n",
       "      <td>0.074381</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.305430</td>\n",
       "      <td>0.536479</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>он</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        я       ты         и    только        не        он\n",
       "я и ты           0.074381  0.30543  0.170275  0.000000  0.000000  0.000000\n",
       "ты и я           0.074381  0.30543  0.170275  0.000000  0.000000  0.000000\n",
       "я, я и только я  0.133886  0.00000  0.102165  0.183258  0.000000  0.000000\n",
       "только не я      0.074381  0.00000  0.000000  0.305430  0.536479  0.000000\n",
       "он               0.000000  0.00000  0.000000  0.000000  0.000000  1.609438"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9076e",
   "metadata": {},
   "source": [
    "## Задание 3 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e25357",
   "metadata": {},
   "source": [
    "Обучите 2 любых разных классификатора из семинара. Предскажите токсичность для текстов из тестовой выборки (используйте одну и ту же выборку для обоих классификаторов) и найдите 10 самых токсичных для каждого из классификаторов. Сравните получаемые тексты - какие тексты совпадают, какие отличаются, правда ли тексты токсичные?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de962ad",
   "metadata": {},
   "source": [
    "Требования к моделям:   \n",
    "а) один классификатор должен использовать CountVectorizer, другой TfidfVectorizer  \n",
    "б) у векторазера должны быть вручную заданы как минимум 5 параметров (можно ставить разные параметры tfidfvectorizer и countvectorizer)  \n",
    "в) у классификатора должно быть задано вручную как минимум 2 параметра (по возможности)  \n",
    "г)  f1 мера каждого из классификаторов должна быть минимум 0.75  \n",
    "\n",
    "*random_seed не считается за параметр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "745bd58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код\n",
    "data = pd.read_csv(r\"C:\\Users\\marus\\Documents\\Вышка\\compling_nlp_hse_course\\notebooks\\bow\\labeled.csv\")\n",
    "\n",
    "train, test = train_test_split(data, test_size = 0.1, shuffle = True)\n",
    "train.reset_index(inplace = True)\n",
    "test.reset_index(inplace = True)\n",
    "\n",
    "y_train = train.toxic.values\n",
    "y_test = test.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90311fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.88      0.88       930\n",
      "         1.0       0.78      0.79      0.78       512\n",
      "\n",
      "    accuracy                           0.84      1442\n",
      "   macro avg       0.83      0.83      0.83      1442\n",
      "weighted avg       0.84      0.84      0.84      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 - векторизатор CountVectorizer, модель - Наивный Байес\n",
    "\n",
    "vectorizer1 = CountVectorizer(max_df = 0.9, min_df = 5, max_features = 5000, analyzer='word', ngram_range=(1,1))\n",
    "\n",
    "X_train1 = vectorizer1.fit_transform(train.comment)\n",
    "X_test1 = vectorizer1.transform(test.comment) \n",
    "\n",
    "classifier1 = MultinomialNB(alpha=1.5, fit_prior = False)\n",
    "classifier1.fit(X_train1, y_train)\n",
    "\n",
    "y_preds1 = classifier1.predict(X_test1)\n",
    "\n",
    "print(classification_report(y_test, y_preds1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f939c857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.88      0.88       930\n",
      "         1.0       0.77      0.77      0.77       512\n",
      "\n",
      "    accuracy                           0.84      1442\n",
      "   macro avg       0.82      0.82      0.82      1442\n",
      "weighted avg       0.84      0.84      0.84      1442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Векторизатор - Tf-Idf, модель - Логистическая регрессия\n",
    "vectorizer2 = TfidfVectorizer(max_df = 0.9, min_df = 5, max_features = 7000, analyzer='word', ngram_range=(1,1))\n",
    "\n",
    "X_train2 = vectorizer2.fit_transform(train.comment)\n",
    "X_test2 = vectorizer2.transform(test.comment) \n",
    "\n",
    "сlassifier2 = LogisticRegression(C=0.5, class_weight='balanced', max_iter = 1500, random_state=0)\n",
    "сlassifier2.fit(X_train2, y_train)\n",
    "\n",
    "y_preds2 = сlassifier2.predict(X_test2)\n",
    "\n",
    "print(classification_report(y_test, y_preds2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c704d6f",
   "metadata": {},
   "source": [
    "Здесь моя идея была следующей: самые токсичные комментарии - те, у которых вероятность класса 1 (т.е. токсичности) самая высокая, а класса 0 - самая низкая. Ниже я посчитала вероятности для обоих классификаторов, закинула их в датасет, отсортировала по токсичности по убыванию, затем по \"нетоксичности\" по возрастанию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d3596c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas1 = classifier1.predict_proba(X_test1)\n",
    "probas2 = сlassifier2.predict_proba(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60d96d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.98917866e-05, 9.99960108e-01],\n",
       "       [9.95386230e-01, 4.61377004e-03],\n",
       "       [4.13334334e-02, 9.58666567e-01],\n",
       "       ...,\n",
       "       [7.35967941e-02, 9.26403206e-01],\n",
       "       [9.99998456e-01, 1.54354878e-06],\n",
       "       [9.77713975e-01, 2.22860251e-02]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cbf464b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas1_df = pd.DataFrame(probas1, columns = ['0', '1'])\n",
    "probas1_df = probas1_df.sort_values(by = ['1', '0'], ascending = [False, True])\n",
    "probas2_df = pd.DataFrame(probas2, columns = ['0', '1'])\n",
    "probas2_df = probas2_df.sort_values(by = ['1', '0'], ascending = [False, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4925e39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>8.432810e-57</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.480939e-44</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>1.626468e-21</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1.553796e-15</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1369</th>\n",
       "      <td>1.856787e-13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>3.202889e-11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>5.240218e-11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>8.633517e-11</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>1.073710e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>1.503723e-09</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0    1\n",
       "348   8.432810e-57  1.0\n",
       "5     4.480939e-44  1.0\n",
       "336   1.626468e-21  1.0\n",
       "889   1.553796e-15  1.0\n",
       "1369  1.856787e-13  1.0\n",
       "1363  3.202889e-11  1.0\n",
       "161   5.240218e-11  1.0\n",
       "741   8.633517e-11  1.0\n",
       "806   1.073710e-09  1.0\n",
       "1424  1.503723e-09  1.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas1_df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3c91d756",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>0.005520</td>\n",
       "      <td>0.994480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>0.009908</td>\n",
       "      <td>0.990092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>0.010180</td>\n",
       "      <td>0.989820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1164</th>\n",
       "      <td>0.010738</td>\n",
       "      <td>0.989262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>0.013320</td>\n",
       "      <td>0.986680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>0.013997</td>\n",
       "      <td>0.986003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>0.014707</td>\n",
       "      <td>0.985293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>0.017623</td>\n",
       "      <td>0.982377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>0.019553</td>\n",
       "      <td>0.980447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>0.020065</td>\n",
       "      <td>0.979935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1\n",
       "1371  0.005520  0.994480\n",
       "638   0.009908  0.990092\n",
       "806   0.010180  0.989820\n",
       "1164  0.010738  0.989262\n",
       "1071  0.013320  0.986680\n",
       "1039  0.013997  0.986003\n",
       "1424  0.014707  0.985293\n",
       "827   0.017623  0.982377\n",
       "1189  0.019553  0.980447\n",
       "940   0.020065  0.979935"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probas2_df[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c2e45d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Блеаадь, как же обидно когда создаешь тред, пародируешь речь ватников, случайно употребив слово из скрипта, и он скрывается у всех. Пересоздаю. Я много лет тут сижу с вами и обсераю пидорашек хоть я и сам один из них , смеюсь над их смертями, делаю фотожабы с обезьянами. Я-то думал это делают русские русофобы и украинцы, которых процентов 10, поэтому не относился как к чему-то иному как самокритике, самоиронии или справедливому негодованию из-за войны на Донбассе (в конце концов, я считаю украинцев своими братьями и их бугурт небезосновательным). Но меня нахуй осенило когда сначала вернули значки, а потом произошел расстрел мечети в НЗ. Тут до кучи блядских муслимов и они делают это вместе с нами (я думал они слишком беспросветно тупы для того, чтобы интересоваться политикой). Называют русских пидорашками, славщитом. Бляядь, вы действительно думаете вы лучше пидорашек? И вообще славян? Хорошо, я согласен, что кавказские мужчины часто выглядят лучше русских. Но в остальном вы намного хуже даже пидорашек, которые живут окруженные говном и не охуевают, когда с утра, выходя из подъезда на работу, вступают в десятисантиметровую грязь, считая это нормой; которые не имеют никакой солидарности ни по какому признаку, ненавидят друг друга априори, даже своих детей бросают как ёбаные негры, которые скорее сдадут тебя начальству за маячащее вознаграждение (которого им не дадут), если попытаешься подбить людей на забастовку из-за невыплаты зарплаты за полгода; трусливый и терпеливый скот, который барина превозносит и делит людей на касты по признаку наличия и крутости жоповозки; пиздлявые уебки, которые превозносят свой уебищный совок, КОТОРЫЙ ИЗОБРЕЛ ВСЕ В МИРЕ А АМЕРИКАНЦЫ УКРАЛИ!!11, но по факту выясняется, что совок сам практически нихуя не построил и не модернизировал, всё либо по западным проектам с согласия запада, либо с помощью технологического шпионажа. Воистину народ-гной, народ-пидор, народ-мразь, самый подлый народ в мире. Но вы даже на фоне пидорашек выглядите микробами, вы просто ёбаные ничтожества, причем я не пытаюсь вас оскорбить, мои слова действительно отражают вложенный в них изначальный смысл, вы ничто. Да, пидорашки именно такие, какими я описал их выше, они уберхуевые, но они днище цивилизованных народов, тогда как вас к этим цивилизованным народам даже отнести нельзя. Пидорашки хоть что-то изобрели, а если не изобрели, то смогли спиздить и воссоздать технологии и не просто какую-нибудь открывалку банок, а ядерное оружие и энергетику, создали ценящуюся в мире литературу, создали пусть и хуёвую, милитаризованную, построенную на лжи и крови, угнетении и унижении собственных граждан и в конце концов развалившуюся, но сверхдержаву, сделали массу географических открытий и захватили полмира, сейчас воспитывают массу хороших кодеров (правда они скорее воспитываются не благодаря, а вопреки, когда видят какой пиздец вокруг и находят из него такой выход). Не говоря уж об остальных славянах, которых вы тут презрительно называете славщитом и чистильщиками британских туалетов. А по факту поймите, мы-то конечно говно, по сравнению со странами первого мира, с Китаем, ЮК и Японией, даже со странами Южной Америки. Но вы-то блядь даже среди рашкинских регионов самое дно. В прошлом вы (чеченцы, даги и прочие сорта) были просто дикарями, которые нападали на русские караваны и только поэтому вас решили выебать и зохватить. А если говорить о среднеазиатах, которые бугуртят с оккупации, то вы были обычными нищими кочевниками, подтиравшимися камнем и пьющими верблюжью мочу, не имеющими названий народами. Сейчас вас все боятся потому, что если пидорашка сделает в вас пару дырок, защищая себя, то его посадят на бутылку, а если вы убьёте пидорашку, то вам нихуя не будет. И есть мнение, что это сделано специально, чтобы русский воспитывался униженным рабом. Вы унижаете русских детей в школе, потому что вы агрессивные обезьяны, не способные в диалог и возгорающиеся с малейшей искры, и пусть мне тут кто-нибудь заикнется про протопоповскую хуету, дети в школе должны учиться, вдохновляться и выбирать будущую профессию, а не бороться за выживание и дебильный статус в иерархии. И не слушайте дебилов из b , унижение и избиение слабых не делает вас лучше тех, кого вы унижаете, делает защита. Я не люблю выражаться фразами нациков, но вы воистину столетия просто сидели в горах и ебали баранов, вас даже народ-пидор смог захватить. Я знаю, что есть много армянских ученых, музыкантов, известных людей всех сортов, получивших признание в странах первого мира. Я знаю про охуенную грузинскую культуру (которую вы, судя по всему, нагло спиздили в свое время). Но где блядь чеченские я знаю, что вас тут особенно много, поэтому акцентирую внимание именно на вас среди всех муслимских народов СНГ , дагестанские ученые? Открыли дверь? Где государства, культуры? Почему в среднеазиатских парашах работы нет вообще и они все едут уже не в рашку, а в азиатские страны работать заграницу? Хоть что-то есть в вас достойное упоминания, кроме наглости и агрессии? Арийцы, блядь? Самая чистая раса, о чем любят похвастаться чеченцы? Пик 2, вот такие арийцы. Помесь арабов турков, славян и других европеоидов. Алсо, Володин долбит, Пыня хуйло, рашка бантустан, русня народ-гной.\\n',\n",
       "       'Стас, никому, кроме тебя и армии твоих подсосов(которые представляют собой типичный дегенеративный биомусор, ведущийся на любые скандалы-интриги), твои ролики нахуй не нужны. Серьёзно, ты сделал новости с целью показать, что такое говно может делать любой, а аудитория осталась на том же уровне, ведь людям извне ты не интересен. Да ещё и просит не подписываться, чтобы такую-то годноту ложкой хлебать подольше. Ты обосрался, стал посмешищем для абсолютно всех ютуберов, которые не являются полными ебланами. Тот же Хованский не ссыт тебе на ебало только потому, что ты вертишься с ним в одной компании, иногда даже лично пересекаетесь. Приятно было слышать, как он говорил, что отстреливался бы от таких, как ты, из огнестрела, стараясь забрать с собой побольше коммунистов, когда они придут его оаскулачиапть? Он открыто хуесосил людей и за меньшие грехи. Сложи 2 и 2, как он к тебе относится на самом деле. После чего ты сделал ещё более смешной ролик, где истеришь как побитая шлюха во время ПМС. Я ПОДЕБИЛ, А ЕСЛИ ВЫ НЕ ПОНЯЛИ ЭТОГО, ТО ВЫ ТУПЫЕ . Ты мог хотя бы сам его посмотреть перед заливом? Мне даже рофлить над тобой расхотелось, из смешного дегенерата, ты стал жалким дурачком. Это как смеятся над роликами, где контуженные ветераны пытаются ходить под клубную музыку. Над неполноценными смеятся плохо, даже стыдно стало. Я не утрирую. Просто посмотри на себя, Стас. Ну правда. Банишь людей в группе за лвйки и одно упоминание стрима. Ты делаешь всё, в чём самый отбитый и дегенеративный либераст обвиняет совок и сверкаешь разорванным очком. Никто тебя несправедливо не обсирал. Что на стриме по поводу дат, ну ты же сам проебался. На подкасте сообщил, что не будешь стримить. Если ты не был уверен, то зачем это говорить? А если был, почему не сообщил Маргиналу сразу же? Твоё слово в целом не стоит нихуя. Обещаешь не банить-куча удалённых комментов. Обещаешь стрим-не идёшь. Обещаешь что-то ещё, всегда проёбываешься, всё чаще на нарушение обещания тебе нужно в районе секунды-дня. Стоит ли удивлятся, что тебе за это прилетело? Когда то должно было. Ты сам срёшь себе в штаны, не злись, когда на это указывают пальцем.\\n',\n",
       "       'Как известно, у Укр ины (т.е. окр ины), слепленной по пьяни на коленке во 2-м десятилетии XX в., нет истории до XX столетия. Все земли, которые сейчас занимает Укр ина, это русские, румынские, польские и венгерские земли. Напоминаем, что укр инство это сугубо левацкая, антиконсервативная местечково-хуторская идеология, направленная, как и прочие левацкие идеи, на разделение больших наций и поддержание диктата интернацистов. Сторонники бандеровцев (леваков, выступавших за бесклассовое общество и борьбу с капитализмом) и карлика-душителя котов Степана Бандеры, который, как известно, боролся с расизмом, поддерживал Идель-Урал и называл побратимами исламских борцов за свободу из Азербайджана, не пользуются симпатиями у правых европейцев. И это правильно. Попытки заявить о некой отдельной нации неких украинцев это манипуляции, созданные с целью оторвать от русских часть их этнических земель и ослабить в будущем Россию. Только так, чудовищной ложью и тотальной пропагандой, фейковая нация укр инцев , слепленная советскими кукловодами из русских Юга и Киевщины, галичан, поляков, советских румын, славянизированных гуцулов, закарпатских венгров, евреев, татар и многонациональных советских новиопов (а ля Бабченко), может обрести жизнь на русских этнических землях. Разумеется, нет никакого народа укр инцев , как бы одно соседнее failed state ни пыталось их вывести из русских путём обмана, коверканья истории и откровенной фальсификации. Нынешний эксперимент по созданию некой украинской нации можно сравнить разве что с советским экспериментом по созданию нации советской на основании таких же мифов, фейков и откровенного бреда. И маниакальное желание снести все памятники выродку Ленину (Бланку) вас не должно обнадёживать. На смену ему устанавливают памятники такого же левацкого дегенерата-кошкодава Бандеры, чьи руки по локоть в славянской (прежде всего, польской) крови. Заместо совковой лжи про Великую революцию Октября пришла точно такая же наглая ложь про Великую революц ю Г дност абстрагируйтесь от фигуры блогера и посмотрите видео Чем в итоге завершился советский эксперимент, мы все знаем. Ждём закономерного итога эксперимента эльфийского (простите, укр инского ). Разумеется, зомбированные люди будут цепляться до последнего за свои мифы про отельный народ и чужих московитов , но всё это наваждение рано или поздно сгинет, как сгинул Совок со своей мощной идеологией, мифами и фейками.\\n',\n",
       "       'Поясняю за Била. Те, кто его смотрит отморозки, или быдло. Они просто не могут представить, что могут оказаться в такой ситуации. А для них омежки это скот, но когда затрагивают их ЧСВ совершенно безобидным пранком, когда облили водой раздвиженцев, то быдло и отморозки встают в позу и говорят о НАРУШЕНИЕ ГРАНИЦ. Тем, кому припекает омежки, которые из-за низкой самооценки неосознанно представляют себя на месте героев пранка. А потом в своих влажных фантазия избивают этого пранкера или пытают его. А нормальные люди просто это не смотрят. А если и посмотрят то просто скажут, что он дурак. Бил же трус и быдло -- доебывается обычно до слабых, что свойственно гопоте. Очень удивилась, что у него так много лайков на видосах. Ну видимо это те, кто любят Хованского, Мопса и прочих быдлоютуберов. Не удивлюсь, что Била смотрят в основном не из приличных городов, а из всяких мухасрансков. Вывод, русские терпилы будут терпеть до конца. Как лев против или стопхам(который уже окуклился). Включу либераста, как видите это типичный руSSкий мир, мы готовы превозносить любое быдло в герои, а потом оправдывать его. С Билом ниче не случится, я гарантирую это. Он явно не тупой и точно знает, где граница дозволенного, то есть мы не увидим его с ЛГБТ-флагом в Чечне, например. Так что, дорогие руSSкие сосите и дальше хуй и терпите. Мимо проходила\\n',\n",
       "       'А нахуй ты тут персональный чатик устроил, дегенерат? Или ты сейчас каждому работяге, который зайдет в твой обоссаный тред будешь бежать доказывать в чем он не прав и как нужно сделать шоб было всё нормально ? Проблема травлятредов была озвучена ещё задолго до того, как ты и тебе подобные, кукарекающие про школьников, срыватели покровов засунул на этот сайт свой немытый ебальник. Правда для этого люди не разводили шитпостинг на овер 100 постов с переливанием из пустого в порожнее. b для этого подходит идеально - вот там со своими братьями по разуму и разводи драму, хоть на несколько тредов. Я кончил и закурил, можешь теперь маршировать нахуй.\\n',\n",
       "       'Лазурник, ты? Я узнал тебя по твоим шизоидным постам. Помнишь, как ты пытался что то выдавить из себя в Джапари библиотеке, а мы с ассистентом нассали в твоё карри? Ты ещё съел, облизнулся и попросил добавки. А потом сказал, что всю жизнь будешь ненавидеть друзей, но когда ты пошёл на них в рейд, чтобы их съесть, то они пустили тебя по кругу, после чего тебе наложили на анус восемь швов. Как поживаешь, Лазурник?\\n',\n",
       "       'нету. В тех же США большинство негров-мужчин были судимы. Пидораха, ты определись, у тебя речь о мигрантах или таких же коренных полноправных гражданах-неграх? ты еслибыкаешь да баттхертишь оп принёс пруфы скриншот газеты уровня СПИД-инфо хач-вампир выебал ЗЭКА Я же не svin ja чтобы копаться в желтушном мусоре и тащить всё сюда в тред. Вон есть всякие паблики новости русского мира , наслаждайся пруфами там.\\n',\n",
       "       'жид ы захватили власть в россии, уничтожили рускость... русские даже бояться говорить что они русские, боле того они считают нормальным что все олигархи жид ы, вся финансово-экономическая политика под жид ами всё сми и телевидение под жид ами у русских нет ничего, они всё отдали без боя и не желают ничего себе возвращать...\\n',\n",
       "       'а ты че? самый умный чтоли? хуй соси, губой тряси! даун ты тупорылый, блядь!!!!\\n',\n",
       "       'Нахуй иди чмо ебаное, рот твой ебал. Говна поешь, быдло\\n'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Самые токсичные комментарии по версии 1-го классификатора\n",
    "\n",
    "test.loc[probas1_df[0:10].index, 'comment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "014efa2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ты бы и к собаке пристроился.\\n',\n",
       "       'БЕЛАРУСЬ, БЛЯТЬ, БЕЛАРУСЬ. СПИДОРСВИН, БЛЯТЬ. НЕПРОБИВАЕМАЯ ХОХЛИНА, СУКА. Какие-же хохлы дененераты, пиздец просто.\\n',\n",
       "       'а ты че? самый умный чтоли? хуй соси, губой тряси! даун ты тупорылый, блядь!!!!\\n',\n",
       "       'Ты отвечаешь педрюхе, ты хейпостишь других анонов, ты получаешь тот тред который заслуживаешь.',\n",
       "       'За то что ты дерейлаешь такой выгодный для вас тред в сторону хохлов, тебя куратор без смазки выебет. Вон он уже идёт.\\n',\n",
       "       'От того то ты назовёшь хохла пидооашкой, пидорахой ты быть не перестанешь, это как с перефорсом няш , от того что ты ими хохлов называешь, 200 летие свинячей истории русни не пропадёт.\\n',\n",
       "       'Нахуй иди чмо ебаное, рот твой ебал. Говна поешь, быдло\\n',\n",
       "       'А он тебя не кормит)? Может тебе котлеток выслать?\\n',\n",
       "       'Ты понимаешь что ты шизоид?\\n',\n",
       "       'не гейское. а почему ты полумал о геях?\\n'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Самые токсичные комментарии по версии 2-го классификатора\n",
    "\n",
    "test.loc[probas2_df[0:10].index, 'comment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b06d3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "а ты че? самый умный чтоли? хуй соси, губой тряси! даун ты тупорылый, блядь!!!!\n",
      "\n",
      "Нахуй иди чмо ебаное, рот твой ебал. Говна поешь, быдло\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Общие комментарии\n",
    "for i in test.loc[probas1_df[0:10].index, 'comment'].values:\n",
    "    for j in test.loc[probas2_df[0:10].index, 'comment'].values:\n",
    "        if i == j:\n",
    "            print(i)            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d869b196",
   "metadata": {},
   "source": [
    "**Выводы**\n",
    "\n",
    "В целом, и первый, и второй классификатор выделили действительно токсичные комментарии. \n",
    "\n",
    "Как видно из топ-10, первый классификатор отдал предпочтение более длинным комментариям, скорее всего из-за использования countvectorizer, который считает частотность слов. \n",
    "\n",
    "В целом, по ощущениям, первый классификатор выделил более токсичные комментарии - но это на субъективный взгляд. У них сопоставимые метрики, и комментарии, которые они выделяют как самые токсичные, действительно очень токсичны.\n",
    "\n",
    "Несколько раз пробовала запускать классификаторы, иногда были общие комментарии (до 3-х штук), иногда нет. Интересно, что один раз оба классификатора выделили как токсичную известную копипасту \"Ублюдок, мать твою..\", в которой действительно используется много токсичных слов - наверное, если применять подобный классификатор в реальной жизни, стоит учитывать подобные примеры интернет-творчества, обычно они используются скорее в шутку :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f228c3e",
   "metadata": {},
   "source": [
    "## *Задание 4 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566929b7",
   "metadata": {},
   "source": [
    "Для классификаторов LogisticRegression и Random Forest найдите способ извлечь важность признаков для предсказания токсичного класса. Сопоставьте полученные числа со словами (или нграммами) в словаре и найдите топ - 5 \"токсичных\" слов для каждого из классификаторов. \n",
    "\n",
    "Важное требование: в топе не должно быть стоп-слов. Для этого вам нужно будет правильным образом настроить векторизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f86878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1cf0eed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Стоп-слова\n",
    "nltk.download('stopwords')\n",
    "russian_stopwords = stopwords.words('russian')\n",
    "russian_stopwords\n",
    "\n",
    "extra_stopwords = [\n",
    "    'тебе',\n",
    "    'это'\n",
    "]\n",
    "\n",
    "for word in extra_stopwords:\n",
    "    russian_stopwords.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "15af4e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\marus\\Documents\\Вышка\\compling_nlp_hse_course\\notebooks\\bow\\labeled.csv\")\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.1, shuffle=True)\n",
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)\n",
    "\n",
    "y_train = train.toxic.values\n",
    "y_test = test.toxic.values\n",
    "\n",
    "vec = TfidfVectorizer(stop_words = russian_stopwords)\n",
    "\n",
    "X_train = vec.fit_transform(train.comment)\n",
    "X_test = vec.transform(test.comment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d736d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "clf_imp = clf.feature_importances_\n",
    "clf_imp = pd.DataFrame(clf_imp, columns =['Importance'])\n",
    "\n",
    "clf_indexes = clf_imp.sort_values(by = 'Importance', ascending = False)[:5].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "023c744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "lr_imp = lr.coef_\n",
    "lr_imp = pd.DataFrame(lr_imp[0], columns = ['Importance'])\n",
    "\n",
    "lr_indexes = (lr_imp.sort_values(by = 'Importance', ascending = False)[:5]).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "91c9af49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['хохлы', 'хохлов', 'очень', 'нахуй', 'блядь'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Топ-5 токсичных слов с RandomForestClassifier\n",
    "\n",
    "vec.get_feature_names_out()[clf_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5dbf7ead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['хохлы', 'хохлов', 'нахуй', 'пиздец', 'блядь'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Топ-5 токсичных слов с logistic Regression\n",
    "\n",
    "vec.get_feature_names_out()[lr_indexes]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
