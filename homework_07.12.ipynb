{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371970ff",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Исправление опечаток"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cf8bd",
   "metadata": {},
   "source": [
    "## 1. Доп. ранжирование по вероятности (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6be25c",
   "metadata": {},
   "source": [
    "Дополните get_closest_hybrid_match в семинаре так, чтобы из кандадатов с одинаковым расстоянием редактирования выбиралось наиболее вероятное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104fb326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка импортов\n",
    "\n",
    "import os, re\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import textdistance\n",
    "from difflib import get_close_matches\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0852a287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные\n",
    "\n",
    "corpus = open('wiki_data.txt', encoding='utf8').read()\n",
    "vocab = Counter(re.findall('[а-яА-ЯёЁ]+', corpus.lower()))\n",
    "\n",
    "word2id = list(vocab.keys())\n",
    "id2word = {i:word for i, word in enumerate(vocab)}\n",
    "\n",
    "vec = CountVectorizer(analyzer='char', ngram_range=(1,1))\n",
    "X = vec.fit_transform(vocab)\n",
    "\n",
    "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8e8814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_match_with_metric(text, lookup,topn=20, metric=textdistance.levenshtein):\n",
    "    # Counter можно использовать и с не целыми числами\n",
    "    similarities = Counter()\n",
    "    for word in lookup:\n",
    "        similarities[word] = metric.normalized_similarity(text, word) \n",
    "    return similarities.most_common(topn)\n",
    "\n",
    "# Добавлено косинусное расстояние из семинара\n",
    "def get_closest_match_vec(text, X, vec, topn=20):\n",
    "    v = vec.transform([text])\n",
    "    \n",
    "    similarities = cosine_distances(v, X)[0]\n",
    "    topn = similarities.argsort()[:topn] \n",
    "    \n",
    "    return [(id2word[top], similarities[top]) for top in topn]\n",
    "\n",
    "def get_closest_hybrid_match(text, X, vec, topn=5, metric=textdistance.damerau_levenshtein):\n",
    "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
    "    lookup = [cand[0] for cand in candidates]\n",
    "    closest = get_closest_match_with_metric(text, lookup, topn, metric=metric)\n",
    "    \n",
    "    # Добавила выбор между словами с одинаковым расстоянием\n",
    "    \n",
    "    distances = []\n",
    "    for item in closest:\n",
    "        a = list(item)\n",
    "        a.append(P(item[0]))\n",
    "        distances.append(a)\n",
    "        \n",
    "    total = pd.DataFrame(distances, columns = ['word', 'distance', 'probability'])\n",
    "    total = total.sort_values(by = ['distance', 'probability'], ascending = False).reset_index(drop=True)\n",
    "    \n",
    "    unique_dist = list(total.distance.unique())\n",
    "    the_most_probable = []\n",
    "    for i in range(len(total)):\n",
    "        if total.distance[i] in unique_dist:\n",
    "            the_most_probable.append(total.word[i])\n",
    "            the_most_probable.append(total.distance[i])\n",
    "            unique_dist.remove(total.distance[i])\n",
    "            \n",
    "    return the_most_probable\n",
    "\n",
    "N = sum(vocab.values())\n",
    "def P(word, N=N):\n",
    "    return vocab[word] / N\n",
    "\n",
    "def predict_mistaken(word, vocab):\n",
    "    return 0 if word in vocab else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05d3649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка работы на true и bad\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [token.strip(punctuation) for token in tokens_1]\n",
    "    tokens_2 = [token.strip(punctuation) for token in tokens_2]\n",
    "    \n",
    "    tokens_1 = [token for token in tokens_1 if token]\n",
    "    tokens_2 = [token for token in tokens_2 if token]\n",
    "    \n",
    "    assert len(tokens_1) == len(tokens_2)\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))\n",
    "\n",
    "mistakes = []\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in range(len(true)):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        if predict_mistaken(pair[1], vocab):\n",
    "            pred = cashed.get(pair[1], get_closest_hybrid_match(pair[1], X, vec)[0])\n",
    "            cashed[pair[1]] = pred\n",
    "        else:\n",
    "            pred = pair[1]\n",
    "        \n",
    "            \n",
    "        if pred == pair[0]:\n",
    "            correct += 1\n",
    "        else:\n",
    "            mistakes.append((pair[0], pair[1], pred))\n",
    "        total += 1\n",
    "            \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] != pred:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == pred:\n",
    "                mistaken_fixed += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f488364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['крышка', 0.8333333333333334, 'крка', 0.8, 'корешка', 0.7142857142857143]\n",
      "Процент правильных слов: 84.5 %\n",
      "Процент исправленных ошибок: 44.33 %\n",
      "Процент ошибочно исправленных правильных слов: 9.56 %\n"
     ]
    }
   ],
   "source": [
    "# Пример работы\n",
    "\n",
    "print(get_closest_hybrid_match('кршка', X, vec))\n",
    "\n",
    "# Результаты\n",
    "\n",
    "print(f\"Процент правильных слов: {round((correct/total)*100, 2)} %\")\n",
    "print(f\"Процент исправленных ошибок: {round((mistaken_fixed/total_mistaken)*100, 2)} %\")\n",
    "print(f\"Процент ошибочно исправленных правильных слов: {round((correct_broken/total_correct)*100, 2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4deda88",
   "metadata": {},
   "source": [
    "**Выводы**\n",
    "\n",
    "Для сравнения: результаты с семинара, без ранжирования по вероятности:\n",
    "\n",
    "- Процент правильных слов: 84.2 %\n",
    "- Процент исправленных ошибок: 42.0 %\n",
    "- Процент ошибочно исправленных правильных слов: 9.56 %\n",
    "\n",
    "Таким образом, учитывание вероятности не влияет на ошибочное исправление правильных слов, но на несколько процентов повышает процент исправленных ошибок, так что для улучшения исправления ошибок дополнительное ранжирование по вероятности кажется разумным."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf9985",
   "metadata": {},
   "source": [
    "## 2.  Symspell (5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392cc23",
   "metadata": {},
   "source": [
    "Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Там к словам применяется только одна операция - удаление символа. Описание алгоритма по шагам:\n",
    "\n",
    "1) Составляется словарь правильных слов  \n",
    "2) На основе словаря правильных слов составляется словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово   \n",
    "3) Для выбора исправления для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений, построенного на шаге 2. Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления  \n",
    "4) Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово  \n",
    "\n",
    "\n",
    "Оцените качество полученного алгоритма теми же тремя метриками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96c9fae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код тут"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e09a73",
   "metadata": {},
   "source": [
    "### 1) Составляется словарь правильных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28656386",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter(re.findall('[а-яА-ЯёЁ]+', corpus.lower()))\n",
    "vocab_top = {word:count for word, count in vocab.items() if count > 5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd862399",
   "metadata": {},
   "source": [
    "### 2) На основе словаря правильных слов составляется словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f530c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(word):\n",
    "    N = sum(vocab_top.values())\n",
    "    return np.log(vocab[word] / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35b82ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc53dc8934446818b9ff86d2c0a8fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/68258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wrong_right = {}\n",
    "for word in tqdm(vocab_top.keys()):\n",
    "    splits = [(word[:j], word[j:]) for j in range(len(word) + 1)]\n",
    "    for L, R in splits:\n",
    "        if R:\n",
    "            delete = L + R[1:]\n",
    "            if (delete in wrong_right.keys() and P(word) > P(wrong_right[delete])) or (delete not in wrong_right.keys()):\n",
    "                    wrong_right[delete] = word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5214f450",
   "metadata": {},
   "source": [
    "### 3) Для выбора исправления для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений, построенного на шаге 2. Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления\n",
    "\n",
    "### 4) Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7472c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symspell(word):\n",
    "    variants = delete_letters(word)\n",
    "    if len(variants) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return find_the_best(variants)\n",
    "\n",
    "def delete_letters(word):\n",
    "    splits = [(word[:j], word[j:]) for j in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    variants = []\n",
    "    for example in deletes:\n",
    "        if example in wrong_right.keys():\n",
    "            variants.append(wrong_right[example])\n",
    "    return variants\n",
    "\n",
    "def find_the_best(variants):\n",
    "    max_var = P(variants[0])\n",
    "    best_word = variants[0]\n",
    "    for var in variants:\n",
    "        if P(var) > max_var:\n",
    "            max_var = P(var)\n",
    "            best_word = var\n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "946d0951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "конце\n",
      "шоколад\n"
     ]
    }
   ],
   "source": [
    "# Пример\n",
    "print(symspell(\"сонце\"))\n",
    "print(symspell(\"шокалад\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f874f",
   "metadata": {},
   "source": [
    "### 5) Оцените качество полученного алгоритма теми же тремя метриками.\n",
    "1) процент правильных слов;\n",
    "\n",
    "2) процент исправленных ошибок\n",
    "\n",
    "3) процент ошибочно исправленных правильных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61ec6449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22408b0f58914aac9a7c8398c299d53d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент правильных слов: 36.57 %\n",
      "Процент исправленных ошибок: 17.62 %\n",
      "Процент ошибочно исправленных правильных слов: 60.63 %\n"
     ]
    }
   ],
   "source": [
    "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
    "true = open('correct_sents.txt', encoding='utf8').read().splitlines()\n",
    "\n",
    "def align_words(sent_1, sent_2):\n",
    "    tokens_1 = sent_1.lower().split()\n",
    "    tokens_2 = sent_2.lower().split()\n",
    "    \n",
    "    tokens_1 = [token.strip(punctuation) for token in tokens_1]\n",
    "    tokens_2 = [token.strip(punctuation) for token in tokens_2]\n",
    "    \n",
    "    tokens_1 = [token for token in tokens_1 if token]\n",
    "    tokens_2 = [token for token in tokens_2 if token]\n",
    "    \n",
    "    assert len(tokens_1) == len(tokens_2)\n",
    "    \n",
    "    return list(zip(tokens_1, tokens_2))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "total_mistaken = 0\n",
    "mistaken_fixed = 0\n",
    "\n",
    "total_correct = 0\n",
    "correct_broken = 0\n",
    "\n",
    "cashed = {}\n",
    "for i in tqdm(range(len(true))):\n",
    "    word_pairs = align_words(true[i], bad[i])\n",
    "    for pair in word_pairs:\n",
    "        \n",
    "        predicted = cashed.get(pair[1], symspell(pair[1]))\n",
    "        cashed[pair[1]] = predicted\n",
    "        \n",
    "        \n",
    "        if predicted == pair[0]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        if pair[0] == pair[1]:\n",
    "            total_correct += 1\n",
    "            if pair[0] !=  predicted:\n",
    "                correct_broken += 1\n",
    "        else:\n",
    "            total_mistaken += 1\n",
    "            if pair[0] == predicted:\n",
    "                mistaken_fixed += 1\n",
    "                \n",
    "print(f\"Процент правильных слов: {round((correct/total)*100, 2)} %\")\n",
    "print(f\"Процент исправленных ошибок: {round((mistaken_fixed/total_mistaken)*100, 2)} %\")\n",
    "print(f\"Процент ошибочно исправленных правильных слов: {round((correct_broken/total_correct)*100, 2)} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b22a1a",
   "metadata": {},
   "source": [
    "**Выводы**\n",
    "\n",
    "Ожидаемо, результаты работы алгоритма ниже, чем у алгоритма Норвига. Так, например, процент исправленных ошибок в 3 раза ниже, чем в алгоритме Норвига (17,6 против 51,2), что совпадает с интуитивными предположениями - в алгоритме Symspell из 4-х возможных вариантов изменений используется только один - удаление, так что ошибки, исправляемые перестановкой, заменой или вставкой не исправляются."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292d96d",
   "metadata": {},
   "source": [
    "## *3. Чтение. (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee4b28f",
   "metadata": {},
   "source": [
    "Прочитайте эту главу в книге Speech and Language Processing - https://web.stanford.edu/~jurafsky/slp3/2.pdf .\n",
    "Ответьте на следующий вопрос:\n",
    "\n",
    "1. Что такое Byte-Pair Encoding (опишите по-русски, как минимум 10 предложениями)?\n",
    "\n",
    "*Это задание не связано напрямую с исправлением опечаток, но это важная тема, к которой мы вернемся в конце курса"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f23377",
   "metadata": {},
   "source": [
    "   **Byte-Pair Encoding**, или **кодирование пар байтов** – это один из алгоритмов, применяющихся для токенизации текстов. В отличие от более простой токенизации разбиением текста по пробелам (или другим символам), при кодировании пар байтов токены не всегда равны словам, что может быть полезно при работе с редкими или несуществующими словами.\n",
    "    \n",
    "   Алгоритм состоит из двух основных шагов. На первом шаге составляется словарь. Изначально он состоит из символов длины 1, например, букв и символа конца слова. Алгоритм проходит по учебному корпусу и считает частотность всех стоящих рядом символов. Самая часто встречающаяся пара символов добавляется в словарь как один символ, после чего в учебном корпусе все эти пары символов заменяются одним, объединенным символом. Затем алгоритм продолжает проходить по учебному корпусу, находя все новые сочетания символов. В результате многократного прогона по корпусу составляется окончательный словарь, в котором к изначальным единичным символам добавлены все возможные сочетания символов, от самых коротких (например, приставок или постфиксов) до полноценных слов.\n",
    "   \n",
    "   На следующем шаге применяется парсер токенов, который использует словарь, составленный на первом шаге, для работы с тестовыми данными. Он проходит по тестовым данным в том же порядке, разделяя их на символы в том порядке, в каком эти символы добавлялись в словарь. В результате работы такого алгоритма токенизации большая часть слов из тестовых данных должна быть представлена как полноценные символы (одно слово = один символ), и лишь редкие, незнакомые алгоритму, слова как части: например, символ-приставка + символ-корень + символ-окончание.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
