{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piratesdragon/homework_actual_problems/blob/main/homework_PEFT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14be5b3",
      "metadata": {
        "id": "e14be5b3"
      },
      "source": [
        "## Задание 1.\n",
        "\n",
        "Дообучите языковую модель на датасете инструкций, используя LoRA. Проверьте, что дообученная модель отличается от изначальной - сгенерируйте продолжения для одних и тех же промптов и сравните результаты.\n",
        "\n",
        "Вы можете взять за основу код семинара PEFT, изменив датасет цитат на датасет инструкций (можно просто скопировать из семинара про General_instruct_fine-tuning). \n",
        "Можно использовать alpaca_dataset, датасет Dolly 2 или ваш собственный переведенный датасет (или все вместе). \n",
        "Важно использовать модель с большим количеством параметров (относительно семинара по General instruct fine-tuning). Например, можно попробовать OPT-1.3b,OPT-2.7b или ai-forever/rugpt3large_based_on_gpt2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "14a624a4",
      "metadata": {
        "id": "14a624a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ecd2e61-9f84-4e65-df1d-853221275d7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git\n",
        "\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-1.3b\", \n",
        "    load_in_8bit=True, \n",
        "    device_map='auto',\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7EDeUoY572T",
        "outputId": "8b7f610d-3d21-4278-f994-9a34083c4f0e"
      },
      "id": "x7EDeUoY572T",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()"
      ],
      "metadata": {
        "id": "XjWhJ_p86AGP"
      },
      "id": "XjWhJ_p86AGP",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "TyNXCUU36CXr"
      },
      "id": "TyNXCUU36CXr",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "r2aHmAkM6FK9"
      },
      "id": "r2aHmAkM6FK9",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model \n",
        "\n",
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32, \n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "Hr6kBL776HtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e954543-995d-4947-e521-56343da74d7a"
      },
      "id": "Hr6kBL776HtA",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 6291456 || all params: 1322049536 || trainable%: 0.47588655558516074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Датасет"
      ],
      "metadata": {
        "id": "wDpR25hr0oEp"
      },
      "id": "wDpR25hr0oEp"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
        "import copy\n",
        "import logging\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "import json\n",
        "import torch\n",
        "import transformers\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import Trainer\n",
        "\n",
        "data = json.load(open('alpaca_data.json'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df06RBQB0rTS",
        "outputId": "7e88d1a8-f898-405b-e109-fcd9243bd5c4"
      },
      "id": "Df06RBQB0rTS",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-06 07:51:26--  https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22773992 (22M) [text/plain]\n",
            "Saving to: ‘alpaca_data.json.3’\n",
            "\n",
            "alpaca_data.json.3  100%[===================>]  21.72M   121MB/s    in 0.2s    \n",
            "\n",
            "2023-05-06 07:51:26 (121 MB/s) - ‘alpaca_data.json.3’ saved [22773992/22773992]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "DEFAULT_EOS_TOKEN = \"</s>\"\n",
        "DEFAULT_BOS_TOKEN = \"</s>\"\n",
        "DEFAULT_UNK_TOKEN = \"</s>\"\n",
        "PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
        "    ),\n",
        "}\n",
        "\n",
        "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
        "    \"\"\"Tokenize a list of strings.\"\"\"\n",
        "    tokenized_list = [\n",
        "        tokenizer(\n",
        "            text,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=tokenizer.model_max_length,\n",
        "            truncation=True,\n",
        "        )\n",
        "        for text in strings\n",
        "    ]\n",
        "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
        "    input_ids_lens = labels_lens = [\n",
        "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
        "    ]\n",
        "    return dict(\n",
        "        input_ids=input_ids,\n",
        "        labels=labels,\n",
        "        input_ids_lens=input_ids_lens,\n",
        "        labels_lens=labels_lens,\n",
        "    )\n",
        "\n",
        "def preprocess(\n",
        "    sources: Sequence[str],\n",
        "    targets: Sequence[str],\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        ") -> Dict:\n",
        "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
        "    examples = [s + t for s, t in zip(sources, targets)]\n",
        "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
        "    input_ids = examples_tokenized[\"input_ids\"]\n",
        "    labels = copy.deepcopy(input_ids)\n",
        "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
        "        label[:source_len] = IGNORE_INDEX\n",
        "    return dict(input_ids=input_ids, labels=labels)\n",
        "\n",
        "class SupervisedDataset(Dataset):\n",
        "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
        "        super(SupervisedDataset, self).__init__()\n",
        "        logging.warning(\"Loading data...\")\n",
        "        list_data_dict = json.load(open(data_path))\n",
        "\n",
        "        logging.warning(\"Formatting inputs...\")\n",
        "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
        "        sources = [\n",
        "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
        "            for example in list_data_dict\n",
        "        ]\n",
        "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
        "\n",
        "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
        "        data_dict = preprocess(sources, targets, tokenizer)\n",
        "\n",
        "        self.input_ids = data_dict[\"input_ids\"]\n",
        "        self.labels = data_dict[\"labels\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
        "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForSupervisedDataset(object):\n",
        "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
        "\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
        "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
        "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
        "        )\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
        "        return dict(\n",
        "            input_ids=input_ids,\n",
        "            labels=labels,\n",
        "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        )"
      ],
      "metadata": {
        "id": "VbbC457x3zIx"
      },
      "id": "VbbC457x3zIx",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    \"facebook/opt-1.3b\",\n",
        "    cache_dir=\"huggingface_cache\",\n",
        "    model_max_length=512,\n",
        "    padding_side=\"right\",\n",
        "    use_fast=False,\n",
        ")"
      ],
      "metadata": {
        "id": "8hzRLEzK4NaY"
      },
      "id": "8hzRLEzK4NaY",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=model, \n",
        "    train_dataset= SupervisedDataset(tokenizer=tokenizer, data_path=\"alpaca_data.json\"),\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4, \n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100, \n",
        "        max_steps=400, \n",
        "        learning_rate=2e-4, \n",
        "        fp16=True,\n",
        "        logging_steps=1, \n",
        "        output_dir='outputs'\n",
        "    ),\n",
        "    data_collator=DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAZ9NSJsDoNW",
        "outputId": "77c5fa8d-8528-490c-db41-183859185878"
      },
      "id": "XAZ9NSJsDoNW",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Loading data...\n",
            "WARNING:root:Formatting inputs...\n",
            "WARNING:root:Tokenizing inputs... This may take some time...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z3CsskXX5IpE",
        "outputId": "7ccbc905-ea53-4dcc-a33b-3da32963ce0c"
      },
      "id": "Z3CsskXX5IpE",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [400/400 31:18, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.194700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.275200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.798600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.061400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.004800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.924100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.107100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.981700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.264800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.078500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>2.030400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>2.084600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>2.096700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>2.123300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>2.130400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.919600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.751800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>2.079700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.993400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.010600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.687200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>2.072000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>2.037000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.849800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.993600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.897300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.687000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>1.809600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>2.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.735700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.999200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.843900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.981800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.688500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.711400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.880300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.886500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.784900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>1.945800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.808900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>1.918000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>2.042100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.646100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>1.879500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.695700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.845100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.638200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.810200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>1.475900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.882400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>1.764900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>1.945200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>1.757900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>1.682800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>1.940600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>1.967000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>1.699400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>1.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>1.818200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.610800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>1.647200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.133400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>1.753300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>1.769100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>1.664800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>1.590800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>1.803000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>1.684700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.591900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>1.853900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>1.835600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>1.685800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>1.651900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.574000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>1.906000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>1.649200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>1.680300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>1.798600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>1.744000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.825500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.661500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>1.968200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>1.605900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>1.653300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>1.744100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>1.730200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>1.872500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>1.717700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>1.551900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>1.600300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>1.609300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.833200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>1.895600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>1.632100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>1.542600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>1.718200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>1.674700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.528500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>1.928600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.586400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>2.019800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>1.542100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>1.627800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>1.253200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>1.716800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>1.609200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>1.815300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>1.558200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>1.658100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>1.829100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>1.561900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>1.639400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>1.718600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>1.545000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>1.658300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>1.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>1.652700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>1.833200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>1.850700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.613800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>1.885600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>1.403600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>1.857800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>1.721500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>1.809300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>1.767600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>1.760500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>1.664700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>1.542400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>1.637400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>1.998000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>1.714900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>1.943100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>1.665300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>1.685300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>1.797000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>1.686700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>1.862600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>1.558200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.912300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>1.572800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>1.552300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>1.828300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>1.677700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>1.664600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>1.604100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>1.913700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>1.584900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>1.666200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.430500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>1.586200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>1.763400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>2.128100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>1.779700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>1.671500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>1.714400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>1.546300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>1.818000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>1.914200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.536400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>1.776000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>1.713800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>1.667200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>1.720400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>1.813100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>1.318800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>1.534200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>1.644300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>1.695400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>1.962200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>1.683900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>1.728000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>1.545000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>1.760700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>1.497200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>1.652600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>1.772500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>1.922000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>1.782500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.676600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>1.746600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>1.619800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>1.625200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>1.807300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>1.725200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>1.583600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>1.785400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>1.633200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>1.752100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>1.659900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>1.478700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>1.461900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>1.776300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>1.491500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>2.048000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>1.721300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>1.487100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>1.959300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>1.618200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.224500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>1.584000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>1.779900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>1.720500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>1.582700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>1.796300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>1.846200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>1.693000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>1.500100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>1.731400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>1.716200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>1.430600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>1.690700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>1.826800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>1.726800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>1.747700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>1.586600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>1.637900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>1.607100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>1.795600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.894400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>1.798300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>1.537700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>1.646500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>1.676200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>1.794300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>1.720300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>1.693000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>1.424700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>1.389200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>1.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>1.455300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>2.093700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>1.572200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>1.569800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>1.582700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>1.715400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>1.692200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>1.642900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>1.522400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.636000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>1.452300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>1.632500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>1.819800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>1.585400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>1.719800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>1.704600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>1.694800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>1.513800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>1.856800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.495900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>1.625400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>1.496000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>1.838500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>1.665500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>1.690700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>1.766000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>1.516000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>1.456200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>1.822700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.601100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>1.794800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>1.898800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>1.651600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>1.557100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>1.490800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>1.615300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>1.382800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>1.661500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>1.782900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>1.796500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>1.586700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>1.556500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>1.429000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>1.608000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>1.406800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>1.701000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>1.447000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>1.829500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>1.645900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.765400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>1.822900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>1.688900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>1.402500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>1.672500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>1.589600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>1.589200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>1.853500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>1.834900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>1.631600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>1.645600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>1.870700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>1.641400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>1.621600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>1.805900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>1.503600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>1.518300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>1.736600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>1.809600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>1.690700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.433900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>1.656100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>1.718100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>1.720000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>1.472500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>1.458600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>1.775200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>1.733900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>1.725100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>1.758300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>1.763500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>1.812400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>1.615700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>1.811800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>1.540900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>1.554000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>1.769300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>1.860700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>1.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>1.513900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.645200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>1.714400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>1.533200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>1.775600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>1.659400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>1.623900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>1.671300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>1.299700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>1.776900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>1.682800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>1.421900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>1.514000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>1.575500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>1.714500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>1.886600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>1.644100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>1.547900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>1.824300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>1.455700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>1.454900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.634600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>1.206300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>1.725600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>1.447700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>1.755300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>1.706600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>1.534800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>1.608400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>1.704500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>1.583800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.741200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>1.501900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>1.532600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>1.466500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>1.781800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>1.738300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>1.662200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>1.630600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>1.536000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>1.389400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.879700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>1.661200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>1.775900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>1.718000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>1.519200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>1.748100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>1.472200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>1.581300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>1.891100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>1.341200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>1.522100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>1.677600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>1.668500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>1.384900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>1.658300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>1.523300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>1.542600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>1.471000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>1.445900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>1.624100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.601700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>381</td>\n",
              "      <td>2.024000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>1.776800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>383</td>\n",
              "      <td>1.615900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>1.487900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>1.723300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>1.708200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>387</td>\n",
              "      <td>1.660000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>1.790500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>389</td>\n",
              "      <td>1.713200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>1.628000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>391</td>\n",
              "      <td>1.705400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>1.758100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>393</td>\n",
              "      <td>1.898700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>394</td>\n",
              "      <td>1.964500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>1.527400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>1.614300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>397</td>\n",
              "      <td>1.668000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>398</td>\n",
              "      <td>1.698500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>1.496500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.687300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=400, training_loss=1.705285763144493, metrics={'train_runtime': 1881.9449, 'train_samples_per_second': 3.401, 'train_steps_per_second': 0.213, 'total_flos': 8169890551726080.0, 'train_loss': 1.705285763144493, 'epoch': 0.12})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('opt_1.3_lora')"
      ],
      "metadata": {
        "id": "NxJEnXaAOfem"
      },
      "id": "NxJEnXaAOfem",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Проверка на сгенерированных примерах"
      ],
      "metadata": {
        "id": "PNx_Ed_PWVKu"
      },
      "id": "PNx_Ed_PWVKu"
    },
    {
      "cell_type": "code",
      "source": [
        "# перед запуском этой ячейки нужно перезапустить кернел\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"opt_1.3_lora\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\"facebook/opt-1.3b\", \n",
        "                                             return_dict=True, load_in_8bit=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELIi_82IOi0v",
        "outputId": "a0679278-f34a-479f-cc8b-a90730209b3f"
      },
      "id": "ELIi_82IOi0v",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-qa6s9nbkfkw6 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true'), PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:145: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(text, tokenizer, model):\n",
        "  batch = tokenizer(text, return_tensors='pt').to('cuda')\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=50, temperature=0.0, no_repeat_ngram_size=2)\n",
        "\n",
        "  return tokenizer.decode(output_tokens[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "E-Zjht8BOnVT"
      },
      "id": "E-Zjht8BOnVT",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Примеры без дообучения"
      ],
      "metadata": {
        "id": "rQiGCq17UCtw"
      },
      "id": "rQiGCq17UCtw"
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Give three tips for passing an exam.\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "M0LpRtVWOq7L",
        "outputId": "dbc69640-cf44-4ca9-b660-e54ba5737b6b"
      },
      "id": "M0LpRtVWOq7L",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Give three tips for passing an exam.\\n\\n1. Don’t be afraid to ask questions. If you don‘t know something, ask. You‚ll learn a lot. And you„ll be able to pass the exam with flying colors.2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Who is the author of War and Peace?\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qSmshtrwQVT-",
        "outputId": "c326fd16-2e75-4841-a62f-f02bb24c6e65"
      },
      "id": "qSmshtrwQVT-",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Who is the author of War and Peace?\\n\\nThe author is Leo Tolstoy.\\nI'm not sure if you're asking about the book or the movie. The book is a novel, and the film is based on it.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Name five things that you need to take on holiday\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "h9F3umKnUJU7",
        "outputId": "d7626988-6400-4290-8df9-26fabee29d9e"
      },
      "id": "h9F3umKnUJU7",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Name five things that you need to take on holiday.\\n\\n1. A good book. I’m not talking about a book that”s on the beach, or in the library. You need a good, solid book to read. Something that will keep you entertained, and keep'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Generate a question that could follow the following response: I don't like reading.\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WWc4ANIvUZBt",
        "outputId": "9bcc4177-c2e8-4092-ec8f-e2bb3aebc39a"
      },
      "id": "WWc4ANIvUZBt",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Generate a question that could follow the following response: I don't like reading. I like to listen to music.\\n\\nThe question is: What is the best way to learn to read?\\nAnswer: Read. Read a lot. Listen to a book. Watch a movie. Play a game. Talk to someone who reads\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Recommend me a good science fiction movie.\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rD8DDIOxUjKa",
        "outputId": "3d61ea3c-9fc8-4b5c-d4dc-6d04fddd7851"
      },
      "id": "rD8DDIOxUjKa",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Recommend me a good science fiction movie.\\nThe Martian.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Recommend me a good science fiction movie and tell me about it.\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mnBBHZaHU9sU",
        "outputId": "8e0c7d16-0012-49d3-ebf4-456887f13074"
      },
      "id": "mnBBHZaHU9sU",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Recommend me a good science fiction movie and tell me about it.\\nThe Martian.   It's a great movie, but it's not a science-fiction movie.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Примеры с дообучением"
      ],
      "metadata": {
        "id": "DSIdzmTmUFoz"
      },
      "id": "DSIdzmTmUFoz"
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(model, peft_model_id)\n"
      ],
      "metadata": {
        "id": "JMQiMAJYO_MW"
      },
      "id": "JMQiMAJYO_MW",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Give three tips for passing an exam.\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "0c5cfd40-2543-4fd5-9850-9e738207a5d5",
        "id": "PqPK_WlXWNkd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Give three tips for passing an exam.\\n\\n1. Know the material. The exam is not a guessing game. You should know the content of the exam and be able to answer questions about it. If you don’t know what the questions are about, ask your teacher or'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "id": "PqPK_WlXWNkd"
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Who is the author of War and Peace?\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "5af2cbc6-6306-4a31-daa2-0f49e9716c4f",
        "id": "uaPc8AqcWNke"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Who is the author of War and Peace?\\n\\nThe author is Leo Tolstoy.\\nWar and peace is a novel by Russian author Leo T. TolSTOY. It was first published in 1855 and is considered one of the greatest works of literature in the world.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "id": "uaPc8AqcWNke"
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Name five things that you need to take on holiday\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bc09ccf4-750c-4551-8678-8da8214af830",
        "id": "RbeLW0_JWNkf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Name five things that you need to take on holiday.\\n\\n1. A good book. I love reading and I always have a book with me on the plane. It’s a great way to pass the time and it‘s also a good way of learning new things.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "id": "RbeLW0_JWNkf"
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Generate a question that could follow the following response: I don't like reading.\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "bb53b3e5-1e4a-4317-ba99-6febd6da4cd7",
        "id": "3MSnd-GyWNkf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Generate a question that could follow the following response: I don't like reading. What is the best way to get me to read?\\n\\nAnswer: The best method to encourage reading is to make reading fun. Make reading a game, and make it a reward for good behavior. For example, if you read a book,\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "id": "3MSnd-GyWNkf"
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Recommend me a good science fiction movie.\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c44f0459-2ff2-4801-a6b9-eb4ae06a616c",
        "id": "fgysB-jcWNkf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Recommend me a good science fiction movie.\\nThe Martian.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "id": "fgysB-jcWNkf"
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Recommend me a good science fiction movie and tell me about it.\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "df2ef4d1-4154-4b8e-e2e8-f55127616dd5",
        "id": "WiWSjk7sWNkf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Recommend me a good science fiction movie and tell me about it.\\nThe Martian by Andy Weir. It's a great book and movie.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "id": "WiWSjk7sWNkf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ответы до и после дообучения похожи, но все-таки отличаются, и каежтся, что дообучение помогает получать более точные ответы.\n"
      ],
      "metadata": {
        "id": "ZA-3ANbHhtve"
      },
      "id": "ZA-3ANbHhtve"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}