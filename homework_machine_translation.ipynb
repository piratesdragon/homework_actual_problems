{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 11. Машинный перевод"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yj7aripVIsbG",
      "metadata": {
        "id": "Yj7aripVIsbG"
      },
      "source": [
        "## Задание 1 (6 баллов + 2 доп балла).\n",
        "Нужно обучить трансформер на этом же или на другом корпусе (можно взять другую языковую пару с того же сайте) и оценивать его на всей тестовой выборке (а не на 10 примерах как сделал я). \n",
        "\n",
        "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Подсказка: модель может предсказывать батчами.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение модели"
      ],
      "metadata": {
        "id": "Ta2uLT7HRyPG"
      },
      "id": "Ta2uLT7HRyPG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05d202c4",
      "metadata": {
        "id": "05d202c4"
      },
      "outputs": [],
      "source": [
        "%pip install tokenizers matplotlib scikit-learn\n",
        "\n",
        "import tensorflow as tf\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers import decoders\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я решила выбрать языковую пару английский-испанский:"
      ],
      "metadata": {
        "id": "9H4wDo5pQjYz"
      },
      "id": "9H4wDo5pQjYz"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-es/opus.en-es-train.es\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-es/opus.en-es-train.en\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-es/opus.en-es-test.es\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-es/opus.en-es-test.en"
      ],
      "metadata": {
        "id": "xdEE2eoOXQUy"
      },
      "id": "xdEE2eoOXQUy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents = open('opus.en-es-train.en').read().lower().splitlines()\n",
        "es_sents = open('opus.en-es-train.es').read().lower().splitlines()"
      ],
      "metadata": {
        "id": "evvZbcr9X9MK"
      },
      "id": "evvZbcr9X9MK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# В английском корпусе почему-то одно предложение по ошибке разбилось на 2 и после него поехала вся нумерация + из-за этого корпус был длины 1000001.\n",
        "# Я нашла это предложение и убрала его.\n",
        "\n",
        "en_sents = en_sents[0:947850]+en_sents[947851:]"
      ],
      "metadata": {
        "id": "Mp_qSkcCQ-RS"
      },
      "id": "Mp_qSkcCQ-RS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer(WordPiece(), )\n",
        "tokenizer_en.normalizer = normalizers.Sequence([Lowercase()])\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_en = WordPieceTrainer(\n",
        "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
        "tokenizer_en.train(files=[\"opus.en-es-train.en\"], trainer=trainer_en )\n",
        "\n",
        "tokenizer_es = Tokenizer(WordPiece(), )\n",
        "tokenizer_es.normalizer = normalizers.Sequence([Lowercase()])\n",
        "tokenizer_es.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_es = WordPieceTrainer(\n",
        "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\", \"[START]\", \"[END]\", ])\n",
        "tokenizer_es.train(files=[\"opus.en-es-train.es\"], trainer=trainer_es )"
      ],
      "metadata": {
        "id": "T4HjwWi9ZEuV"
      },
      "id": "T4HjwWi9ZEuV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en.decoder = decoders.WordPiece()\n",
        "tokenizer_es.decoder = decoders.WordPiece()"
      ],
      "metadata": {
        "id": "B45mmqohZnmy"
      },
      "id": "B45mmqohZnmy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, tokenizer, target=False):\n",
        "    if target:\n",
        "        return [tokenizer.token_to_id('[START]')] + tokenizer.encode(text).ids + \\\n",
        "                [tokenizer.token_to_id('[END]')]\n",
        "    else:\n",
        "        return tokenizer.encode(text).ids \n"
      ],
      "metadata": {
        "id": "OocBDcuTZrKn"
      },
      "id": "OocBDcuTZrKn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_en = [encode(t, tokenizer_en) for t in en_sents]\n",
        "X_es = [encode(t, tokenizer_es, True) for t in es_sents]"
      ],
      "metadata": {
        "id": "_U6mWLvHaEsB"
      },
      "id": "_U6mWLvHaEsB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_en = np.max([len(x) for x in X_en])\n",
        "max_len_es = np.max([len(x) for x in X_es])"
      ],
      "metadata": {
        "id": "b-R-xoJnaI2b"
      },
      "id": "b-R-xoJnaI2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Исходный язык')\n",
        "print('50 % текстов <= ', np.percentile([len(x) for x in X_en], 50))\n",
        "print('75 % текстов <= ', np.percentile([len(x) for x in X_en], 75))\n",
        "print('90 % текстов <= ', np.percentile([len(x) for x in X_en], 90))\n",
        "print('95 % текстов <= ', np.percentile([len(x) for x in X_en], 95))\n",
        "print('99 % текстов <= ', np.percentile([len(x) for x in X_en], 99))\n",
        "\n",
        "print('Целевой язык')\n",
        "print('50 % текстов <= ', np.percentile([len(x) for x in X_es], 50))\n",
        "print('75 % текстов <= ', np.percentile([len(x) for x in X_es], 75))\n",
        "print('90 % текстов <= ', np.percentile([len(x) for x in X_es], 90))\n",
        "print('95 % текстов <= ', np.percentile([len(x) for x in X_es], 95))\n",
        "print('99 % текстов <= ', np.percentile([len(x) for x in X_es], 99))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ea0D2SArayXV",
        "outputId": "dd7d493f-b9ae-4542-bc43-882c73ef6cc6"
      },
      "id": "Ea0D2SArayXV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходный язык\n",
            "50 % текстов <=  9.0\n",
            "75 % текстов <=  17.0\n",
            "90 % текстов <=  31.0\n",
            "95 % текстов <=  42.0\n",
            "99 % текстов <=  71.0\n",
            "Целевой язык\n",
            "50 % текстов <=  10.0\n",
            "75 % текстов <=  18.0\n",
            "90 % текстов <=  35.0\n",
            "95 % текстов <=  49.0\n",
            "99 % текстов <=  81.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_en, max_len_es = 45, 49"
      ],
      "metadata": {
        "id": "lGg0y6mYbBH3"
      },
      "id": "lGg0y6mYbBH3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = tokenizer_es.token_to_id('[PAD]')\n"
      ],
      "metadata": {
        "id": "TwS_HAA8bIH0"
      },
      "id": "TwS_HAA8bIH0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_en = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "              X_en, maxlen=max_len_en, padding='post', value=PAD_IDX)\n",
        "\n",
        "X_es_out = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "              [x[1:] for x in X_es], maxlen=max_len_es-1, padding='post', \n",
        "              value=PAD_IDX)\n",
        "\n",
        "X_es_dec = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "              [x[:-1] for x in X_es], maxlen=max_len_es-1, \n",
        "              padding='post', value=PAD_IDX)"
      ],
      "metadata": {
        "id": "pSS3N7wGbLYf"
      },
      "id": "pSS3N7wGbLYf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_en.shape, X_es_out.shape, X_es_dec.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMlXCyWqbhvP",
        "outputId": "2c116eca-6087-4716-f0dc-409478376edd"
      },
      "id": "kMlXCyWqbhvP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000000, 45), (1000000, 48), (1000000, 48))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_en_train, X_en_valid, \n",
        "X_es_dec_train, X_es_dec_valid, \n",
        "X_es_out_train, X_es_out_valid) = train_test_split(X_en, \n",
        "                                                  X_es_dec, \n",
        "                                                  X_es_out, \n",
        "                                                  test_size=0.05)"
      ],
      "metadata": {
        "id": "5n148hXsbWG0"
      },
      "id": "5n148hXsbWG0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    # Считаем скалярное произведение между запросом (query) и ключом (key), транспонируя ключ\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # Получаем глубину (размерность) ключа и преобразуем ее во float\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "\n",
        "    # Делим результат скалярного произведения на квадратный корень из глубины\n",
        "    # Это делается для уменьшения влияния больших значений и стабилизации градиентов во время обучения\n",
        "    logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "    # Если есть маска, применяем ее к логитам, чтобы обнулить нежелательные значения\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "\n",
        "    # Применяем функцию softmax для получения весов внимания\n",
        "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    # Умножаем веса внимания на значения (value) для получения итогового результата\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "MJngJID1hjy7"
      },
      "id": "MJngJID1hjy7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "        super(MultiHeadAttention, self).__init__(name=name)\n",
        "        self.num_heads = num_heads  # количество голов для внимания\n",
        "        self.d_model = d_model  # размерность вектора модели\n",
        "\n",
        "        # Убеждаемся, что размерность модели делится нацело на количество голов\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads  # размерность каждой головы\n",
        "\n",
        "        # Создаем полносвязные слои для запроса, ключа и значения\n",
        "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "        # Создаем последний полносвязный слой\n",
        "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        # Разделяем входные данные на головы\n",
        "        inputs = tf.reshape(\n",
        "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "            'value'], inputs['mask']\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        # Пропускаем запрос, ключ и значение через соответствующие полносвязные слои\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # Разделяем запрос, ключ и значение на головы \n",
        "        # то есть просто разрезаем вектора на num_heads частей \n",
        "        # и сравниваем все части между собой\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # Выполняем механизм внимания с масштабированным скалярным произведением\n",
        "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # Объединяем головы вместе (склеиваем векторы в один)\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "\n",
        "        # Пропускаем объединенное внимание через дополнительный полносвязный слой\n",
        "        # Он просто добавляет сложности нашей модели\n",
        "        outputs = self.dense(concat_attention)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "BtnT07wMhnPz"
      },
      "id": "BtnT07wMhnPz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha = MultiHeadAttention(d_model=10, num_heads=2)"
      ],
      "metadata": {
        "id": "iMq08IwQhqXg"
      },
      "id": "iMq08IwQhqXg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = np.random.normal(size=(1, 2, 3)) "
      ],
      "metadata": {
        "id": "SXrcX155hs-K"
      },
      "id": "SXrcX155hs-K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {'query': inp,'key': inp, 'value': inp, 'mask':None}"
      ],
      "metadata": {
        "id": "lC6PKiExhvdY"
      },
      "id": "lC6PKiExhvdY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = mha(inputs)"
      ],
      "metadata": {
        "id": "FwJhmYZ_hxzU"
      },
      "id": "FwJhmYZ_hxzU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yO_yHC5h0l8",
        "outputId": "0f0befb2-dd98-4b19-8313-1c922e6eeba8"
      },
      "id": "6yO_yHC5h0l8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 2, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = mha.query_dense(inputs['query'])\n",
        "k = mha.query_dense(inputs['key'])\n",
        "v = mha.query_dense(inputs['value'])"
      ],
      "metadata": {
        "id": "yn5q54mVh4Ji"
      },
      "id": "yn5q54mVh4Ji",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qh = mha.split_heads(q, tf.shape(q)[0])\n",
        "kh = mha.split_heads(k, tf.shape(k)[0])\n",
        "vh = mha.split_heads(v, tf.shape(v)[0])"
      ],
      "metadata": {
        "id": "syWihcQTh6xK"
      },
      "id": "syWihcQTh6xK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.transpose(kh).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REL7r2b0h-jH",
        "outputId": "8c554caf-0e09-4d74-f601-af29be515f13"
      },
      "id": "REL7r2b0h-jH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([5, 2, 2, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matmul_qk = tf.matmul(qh, kh, transpose_b=True, )"
      ],
      "metadata": {
        "id": "Jhsnft_EiAxp"
      },
      "id": "Jhsnft_EiAxp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.nn.softmax(matmul_qk, axis=-1).numpy().round(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKx6fmCKiBz0",
        "outputId": "2085787d-aef4-4056-c6dc-d1183f53ea79"
      },
      "id": "pKx6fmCKiBz0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[0.94, 0.06],\n",
              "         [0.68, 0.32]],\n",
              "\n",
              "        [[0.86, 0.14],\n",
              "         [0.66, 0.34]]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(x):\n",
        "    mask = tf.cast(tf.math.equal(x, PAD_IDX), tf.float32)\n",
        "    # (batch_size, 1, 1, sequence length)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "1MCSQ2UaiGUQ"
      },
      "id": "1MCSQ2UaiGUQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(x):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    padding_mask = create_padding_mask(x)\n",
        "    return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "metadata": {
        "id": "kvpVOG_siL_Q"
      },
      "id": "kvpVOG_siL_Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## в туториале от tensorflow принцип работы postional encoding описан подробнее\n",
        "## https://www.tensorflow.org/text/tutorials/transformer\n",
        "## но для семинара это не так важно\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "metadata": {
        "id": "EUgz_DjCiaSr"
      },
      "id": "EUgz_DjCiaSr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    #call_mha\n",
        "    attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "    attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "7lnVdwjgidwS"
      },
      "id": "7lnVdwjgidwS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name=\"encoder\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    #inputs (они тут называются outputs но это просто такой нейминг, \n",
        "    # этот параметр передается в encoder_layer первым и encoder_layer будет считать его inputs)\n",
        "    # outputs он тут называется для удобства, так как он будет перезаписываться\n",
        "    # чтобы на вход следующему блоку подавать уже не эмбединги,\n",
        "    # а то что получится как результат предыдущего блока\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = encoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name=\"encoder_layer_{}\".format(i),\n",
        "        )([outputs, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "i7FTANQPigCy"
      },
      "id": "i7FTANQPigCy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "    attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "    attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "    attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "RoWUImhNiiqn"
      },
      "id": "RoWUImhNiiqn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name='decoder'):\n",
        "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = decoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name='decoder_layer_{}'.format(i),\n",
        "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "5CABokfQikj7"
      },
      "id": "5CABokfQikj7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                max_len,\n",
        "                name=\"transformer\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "    enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "    \n",
        "    look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "    \n",
        "    dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "    enc_outputs = encoder(\n",
        "                          vocab_size=vocab_size[0],\n",
        "                          num_layers=num_layers,\n",
        "                          units=units,\n",
        "                          d_model=d_model,\n",
        "                          num_heads=num_heads,\n",
        "                          dropout=dropout,\n",
        "                          max_len=max_len[0],\n",
        "                        )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    dec_outputs = decoder(\n",
        "                          vocab_size=vocab_size[1],\n",
        "                          num_layers=num_layers,\n",
        "                          units=units,\n",
        "                          d_model=d_model,\n",
        "                          num_heads=num_heads,\n",
        "                          dropout=dropout,\n",
        "                          max_len=max_len[1],\n",
        "                        )(inputs=[dec_inputs, enc_outputs, \n",
        "                                  look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=vocab_size[1], name=\"outputs\")(dec_outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "QSfbiVL7imkB"
      },
      "id": "QSfbiVL7imkB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L  = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none',)\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    loss = L(y_true, y_pred)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y_true, PAD_IDX), tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "lWSmmAGqiovl"
      },
      "id": "lWSmmAGqiovl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# small model\n",
        "NUM_LAYERS = 2\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "\n",
        "UNITS = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=(tokenizer_en.get_vocab_size(),tokenizer_es.get_vocab_size()),\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT,\n",
        "    max_len=[max_len_en, max_len_es])\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model_ruen',\n",
        "                                            monitor='val_loss',\n",
        "                                            verbose=1,\n",
        "                                            save_weights_only=True,\n",
        "                                            save_best_only=True,\n",
        "                                            mode='min',\n",
        "                                            save_freq='epoch')"
      ],
      "metadata": {
        "id": "1z91GCALiq09"
      },
      "id": "1z91GCALiq09",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit((X_en_train, X_es_dec_train), X_es_out_train, \n",
        "             validation_data=((X_en_valid, X_es_dec_valid), X_es_out_valid),\n",
        "             batch_size=500,\n",
        "             epochs=1,\n",
        "             callbacks=[checkpoint]\n",
        "             )\n",
        "    \n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfWN2498iw5h",
        "outputId": "1099091b-8f7e-4fb2-a3a2-4ed168d700c9"
      },
      "id": "gfWN2498iw5h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1900/1900 [==============================] - ETA: 0s - loss: 1.0741 - accuracy: 0.1261\n",
            "Epoch 1: val_loss improved from inf to 0.74641, saving model to model_ruen\n",
            "1900/1900 [==============================] - 1585s 824ms/step - loss: 1.0741 - accuracy: 0.1261 - val_loss: 0.7464 - val_accuracy: 0.1658\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f025e50ff40>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "    input_ids = encode(text, tokenizer_en, target=False)\n",
        "\n",
        "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                                      [input_ids], maxlen=max_len_en, padding='post', \n",
        "                                       # важно не забыть паддинг с нужным id\n",
        "                                       value=PAD_IDX)\n",
        "\n",
        "    \n",
        "    \n",
        "    output_ids = [tokenizer_es.token_to_id('[START]') ]\n",
        "    \n",
        "    pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False).numpy()\n",
        " \n",
        "    \n",
        "    while pred.argmax(2)[0][-1] not in [tokenizer_es.token_to_id('[END]')]:\n",
        "        if len(output_ids) >= max_len_es:\n",
        "            break\n",
        "        # можно занизить скор тэга UNK чтобы он никогда не генерировался\n",
        "        pred[:, :, tokenizer_es.token_to_id('[UNK]')] = -100\n",
        "\n",
        "        output_ids.append(pred.argmax(2)[0][-1])\n",
        "        pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False).numpy()\n",
        "\n",
        "    return tokenizer_es.decode(output_ids[1:], )\n"
      ],
      "metadata": {
        "id": "tJrB72ITi27z"
      },
      "id": "tJrB72ITi27z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"Can I have some wine, please?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Ma60LVLd_foR",
        "outputId": "5ee7d79b-07b8-4469-f3eb-55a2f96ece46"
      },
      "id": "Ma60LVLd_foR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'¿ puedo tomar un vino, por favor?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Оценка модели"
      ],
      "metadata": {
        "id": "5pFB40kbR6xo"
      },
      "id": "5pFB40kbR6xo"
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents_test = open('opus.en-es-test.en').read().lower().splitlines()\n",
        "es_sents_test = open('opus.en-es-test.es').read().lower().splitlines()"
      ],
      "metadata": {
        "id": "6DaB7m1DAy5_"
      },
      "id": "6DaB7m1DAy5_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(en_sents_test), len(es_sents_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AXwrQDuA39C",
        "outputId": "5f79d08b-2596-40ca-89d4-a01e3229b75e"
      },
      "id": "1AXwrQDuA39C",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "NMx61MXVGoo7"
      },
      "id": "NMx61MXVGoo7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations = []\n",
        "\n",
        "for i in tqdm(range(len(en_sents_test))):\n",
        "    translations.append(translate(en_sents_test[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "2499dd9b0d7d416483ce3fe65338e8de",
            "70ac162d26574c55af32f969dd007ca0",
            "989760648389452a8d095b24f3132bc4",
            "4290e6207f884d649509fd4ce379d5e3",
            "79408835624b45e18f726a2b3778a7e2",
            "2a756a556cec45f6b430871737901827",
            "78c60c6b852c4e588a2bae7dce656ddf",
            "0549de3f155e4c7194f74de512a314ce",
            "ffaae95484324c15bbb675f205f7984c",
            "14bdd412335b4bf1a106389595a8d513",
            "c32344a0c528467ab9e8a5f1fa22726b"
          ]
        },
        "id": "XG4MTalKBTM5",
        "outputId": "954283ad-1e2e-4547-a358-ba3f9bd046e4"
      },
      "id": "XG4MTalKBTM5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2499dd9b0d7d416483ce3fe65338e8de"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "-C3RSGTxBfRA"
      },
      "id": "-C3RSGTxBfRA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleus = []\n",
        "\n",
        "for i, t in enumerate(translations):\n",
        "    reference = tokenizer_es.encode(t).tokens\n",
        "    hypothesis = tokenizer_es.encode(es_sents_test[i]).tokens\n",
        "\n",
        "    bleus.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  auto_reweigh=True))"
      ],
      "metadata": {
        "id": "oZgAJAWLBXfV"
      },
      "id": "oZgAJAWLBXfV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFBD5tBUGT_p",
        "outputId": "a6e0421d-ab1b-4fb9-eb59-bfa73903d3e2"
      },
      "id": "RFBD5tBUGT_p",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['si su país se produce ods para este propósito, por favor entra en la columna 6 en la columna 3.',\n",
              " '* juvie the grand hombre, ¿ quién podría ser?',\n",
              " 'el planeta en casa está corriendo.',\n",
              " '¿ no te matar a las chicas?',\n",
              " 'actividades humanitarias, recuperación y desarrollo',\n",
              " 'la delegación de la república árabe siria oyó estrechamente estrechamente con la declaración realizada por el representante de la república de la república de china, y nosotros hemos asociado a nosotros.',\n",
              " 'tendrás que abordar cualquier pregunta a mi oficial.',\n",
              " \"y estábamos acercando cuatro o ' reloj, nuestro tiempo... estaba sobre un minuto... y la desesperación se instalaron sobre la sala.\",\n",
              " 'así que nunca cuestionarías.',\n",
              " 'hamdan llegó a turakk en la eve de la guerra, un año más tarde que stalin. “ lo que paz y silencio aquí fue liberado.']"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(sum(bleus)/len(bleus))*100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6RnLHlQBY3t",
        "outputId": "4e1b48b5-438b-4024-95cf-9492d5069182"
      },
      "id": "b6RnLHlQBY3t",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17.52958891049895"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5aa93d6",
      "metadata": {
        "id": "b5aa93d6"
      },
      "source": [
        "\n",
        "## Задание 2 (2 балла).\n",
        "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/10.pdf \n",
        "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как его применить к паре en-ru на данных из семинара. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*По ссылке выше другая глава, глава с машинным переводом здесь*: https://web.stanford.edu/~jurafsky/slp3/13.pdf"
      ],
      "metadata": {
        "id": "-oKloSJE6q4-"
      },
      "id": "-oKloSJE6q4-"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backtranslation** (обратный перевод) - это способ увеличить данные для перевода, особенно полезный для малоресурсных языков. Он заключается в следующем: сначала на имеющихся параллельных данных обучается модель, которая позволяет переводить с target языка на source, затем с помощью этой модели переводится некий объем текстов на target языке. Получившиеся данные добавляются к уже существующим данным source языка, и уже на этих объединенных данных обучается модель для перевода с source языка на target. Это позволяет получить больше данных, что особенно полезно, когда данных на одном языке (на target) значительно больше, чем на другом.\n",
        "\n",
        "Допустим, мы хотим применить его к паре en-ru. Наша финальная задача в семинаре - переводить текст с английского на русский, то есть в данном случае английский - это source language, русский - target. Для начала мы, на имеющихся параллельных данных, обычим модель для перевода с русского на английский. Затем применим эту модель к русскоязычным текстам (например, новостям или художественным), которых нет в этом параллельном датасете. Получившиеся предложения добавим к параллельному датасету, и уже на этих расширенных данных обучим модель для перевода с английского на русский.\n"
      ],
      "metadata": {
        "id": "gUndi8o7JrRD"
      },
      "id": "gUndi8o7JrRD"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2499dd9b0d7d416483ce3fe65338e8de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70ac162d26574c55af32f969dd007ca0",
              "IPY_MODEL_989760648389452a8d095b24f3132bc4",
              "IPY_MODEL_4290e6207f884d649509fd4ce379d5e3"
            ],
            "layout": "IPY_MODEL_79408835624b45e18f726a2b3778a7e2"
          }
        },
        "70ac162d26574c55af32f969dd007ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a756a556cec45f6b430871737901827",
            "placeholder": "​",
            "style": "IPY_MODEL_78c60c6b852c4e588a2bae7dce656ddf",
            "value": "100%"
          }
        },
        "989760648389452a8d095b24f3132bc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0549de3f155e4c7194f74de512a314ce",
            "max": 2000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffaae95484324c15bbb675f205f7984c",
            "value": 2000
          }
        },
        "4290e6207f884d649509fd4ce379d5e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14bdd412335b4bf1a106389595a8d513",
            "placeholder": "​",
            "style": "IPY_MODEL_c32344a0c528467ab9e8a5f1fa22726b",
            "value": " 2000/2000 [50:20&lt;00:00,  1.36s/it]"
          }
        },
        "79408835624b45e18f726a2b3778a7e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a756a556cec45f6b430871737901827": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78c60c6b852c4e588a2bae7dce656ddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0549de3f155e4c7194f74de512a314ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffaae95484324c15bbb675f205f7984c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14bdd412335b4bf1a106389595a8d513": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c32344a0c528467ab9e8a5f1fa22726b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}